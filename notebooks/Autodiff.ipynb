{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff\n",
    "\n",
    "In this short notebook we'll look at the auto-differentiation provided by Tensorflow.\n",
    "\n",
    "**Autodiff** means automatic differentiation. In other words, when writing neural networks in Tensorflow, (or in Keras, which is a user interface layer on top of TF), we just need to write the network. The gradient (ie derivative), including back-propagation, is automatically calculated internally. \n",
    "\n",
    "To use Tensorflow, or Keras (or PyTorch, which has the same thing), we don't really need to know how this works internally. So, this notebook is optional, and we're not covering this directly in lectures. But it is pretty interesting and useful to understand at a deeper level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we have a very simple linear model:\n",
    "\n",
    "$\\hat{y} = wx$\n",
    "\n",
    "where $w$ and $x$ are just scalars, but $w$ is a parameter of the model and $x$ is some input data.\n",
    "\n",
    "Let's suppose further that we will use a squared error loss:\n",
    "\n",
    "$$L = (y - \\hat{y})^2 = y^2 - 2 y \\hat{y} + \\hat{y}^2 = y^2 - 2 y w x + w^2x^2$$\n",
    "\n",
    "where again $y$ is just a scalar. (We didn't bother writing $\\sum$ because we'll only consider one training case $(x, y)$, and remember, everything is a scalar.)\n",
    "\n",
    "Now in order to minimise the loss $L$ by optimising the weight $w$ of this model, we need the gradient of $L$ with respect to $w$: $$\\frac{dL}{dw} = -2xy + 2wx^2$$\n",
    "\n",
    "**Exercise**: check the derivative above.\n",
    "\n",
    "Let's choose some arbitrary values for $y$, $w$, and $x$, and then use Tensorflow to calculate the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2.0 # target y value\n",
    "x = 3.0 # input data\n",
    "w = tf.Variable(1.) # initial value of the weight w, stored as a Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have wrapped $w$ up in a `tf.Variable`. This tells Tensorflow it is to be used as a parameter of the model, which can change.\n",
    "\n",
    "Now we're going to use the Tensorflow `tf.GradientTape`. This is the object used behind the scenes by TF to track computations whose gradients will be needed. **In typical TF code, the Gradient Tape is behind the scenes: we don't usually interact with it directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape: # set up the tape\n",
    "    tape.watch(w) # tell the tape that w is a parameter we might need the gradient of\n",
    "    yhat = x * w # run the model\n",
    "    L = (y - yhat) ** 2 # calculate the loss\n",
    "    dL_dw = tape.gradient(L, w) # find the gradient, dL/dw\n",
    "    print(dL_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: this has printed the value $\\frac{dL}{dw} = 6.0$. Look back at our calculation for $\\frac{dL}{dw}$: is the result 6.0 correct?\n",
    "\n",
    "Now let's convince ourselves a little bit more. We'll actually use the gradient to optimise.\n",
    "\n",
    "By looking at the initial numbers, we should see that the optimum value for $w$ is $w=2/3$ (**Exercise**: check this). So, let's set up a learning rate and an optimisation loop which uses the gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940 1.000 6.000\n",
      "0.891 0.672 4.920\n",
      "0.850 0.452 4.034\n",
      "0.817 0.304 3.308\n",
      "0.790 0.204 2.713\n",
      "0.768 0.137 2.224\n",
      "0.750 0.092 1.824\n",
      "0.735 0.062 1.496\n",
      "0.723 0.042 1.226\n",
      "0.712 0.028 1.006\n",
      "0.704 0.019 0.825\n",
      "0.697 0.013 0.676\n",
      "0.692 0.009 0.555\n",
      "0.687 0.006 0.455\n",
      "0.684 0.004 0.373\n",
      "0.681 0.003 0.306\n",
      "0.678 0.002 0.251\n",
      "0.676 0.001 0.206\n",
      "0.674 0.001 0.169\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 # learning rate\n",
    "tolerance = 0.001\n",
    "\n",
    "y = 2.0 # target y value\n",
    "x = 3.0 # input data\n",
    "w = tf.Variable(1.) # initial value of the weight w, stored as a Variable\n",
    "\n",
    "\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(w)\n",
    "        yhat = x * w\n",
    "        L = (y - yhat) ** 2\n",
    "    dL_dw = tape.gradient(L, w)\n",
    "    w = w - lr * dL_dw # take one learning step. notice w will still be a tf.Variable after this\n",
    "    print(f\"{w.numpy():.3f} {L:.3f} {dL_dw:.3f}\")\n",
    "    if tf.abs(L) < tolerance: # if the loss is small, we quit\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $w$ approaches the correct value. So, it looks like things are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try setting `lr` larger, eg `lr = 0.2`. What happens?\n",
    "\n",
    "**Reminder**: typically, when writing Keras code using `Sequential`, we don't have to think about the gradient tape or autodiff. We just specify the loss for our model, and Keras sets up the gradient tape correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. It seems ugly to make a new instance of the tape in every iteration, as we do above. However, it is done this way eg in: https://www.tensorflow.org/guide/core/logistic_regression_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
